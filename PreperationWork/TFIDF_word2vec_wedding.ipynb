{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the ususal stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib as jl\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2VecVocab\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "def remove_ip(s):\n",
    "    # Replace all ip adresses with '<ip>' tag\n",
    "    ip_regexp = r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\"\n",
    "    return re.sub(ip_regexp, '<ip>', s)\n",
    "def remove_email(s):\n",
    "    # Replace all email adresses with '<email>' tag\n",
    "    email_regexp = r\"([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})\"\n",
    "    return re.sub(email_regexp, '<email>', s)\n",
    "def remove_mailto(s):\n",
    "    # Replace all \"<mailto:<email>>\" with <email>. Email adresses should be replaced by remove_email first.\n",
    "    return s.replace(\"<mailto:<email>>\", \"<email>\")\n",
    "def remove_url(s):\n",
    "    # Replace all url's with '<url>' tag\n",
    "    url_regexp = r\"((http|ftp|https):\\/\\/)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "    s = re.sub(url_regexp, '<url>', s)\n",
    "    # Sometimes url's are inside <> so we need to replace <<url>> with <url>\n",
    "    return s.replace(\"<<url>>\", \"<url>\")\n",
    "def remove_punc(s, exceptions):\n",
    "    # Remove all punctuation from string with exceptions in list exceptions\n",
    "    remove = string.punctuation\n",
    "    for exception in exceptions:\n",
    "        remove = remove.replace(exception, \"\")\n",
    "    # Create the pattern\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "\n",
    "    return re.sub(pattern, \"\", s)\n",
    "def remove_custom_stopwords(s, stopwords):\n",
    "    for stopword in stopwords:\n",
    "        s = s.replace(stopword, \"\")\n",
    "    return s\n",
    "def lower_case(s):\n",
    "    return s.lower()\n",
    "def preprocess_sentence_fn(s):\n",
    "    # Preprocess a single sentence to a list of tokens\n",
    "    punc_exceptions = ['<', '>']\n",
    "    custom_stopwords = ['dear', 'sincerely', 'thanks', 'yours', 'regards']\n",
    "    filters = [lower_case,\n",
    "               remove_ip,\n",
    "               remove_email,\n",
    "               remove_mailto,\n",
    "               #remove_url,\n",
    "               lambda x: remove_punc(x, punc_exceptions),\n",
    "               remove_stopwords,\n",
    "               lambda x: remove_custom_stopwords(x, custom_stopwords),\n",
    "               strip_multiple_whitespaces,\n",
    "               stem_text,\n",
    "               strip_numeric]\n",
    "    out = preprocess_string(s, filters=filters)\n",
    "    return out\n",
    "def preprocess_docs_fn(docs):\n",
    "    # Apply preprocess_sentence_fn to a list of sentances (docs) to get a list of lists\n",
    "    return [preprocess_sentence_fn(s) for s in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "ticket_dat = pd.read_csv('../data/12-04-ticket_dat.csv')\n",
    "faq_dat = pd.read_csv('../data/12-04-faq_dat.csv')\n",
    "# Replace the NaNs\n",
    "ticket_dat.fillna('', inplace=True)\n",
    "faq_dat.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAQ question\n",
    "faq_ques = list(faq_dat.question)\n",
    "n_faq_ques = len(faq_ques)\n",
    "# FAQ answer\n",
    "faq_ans = list(faq_dat.answer_title + \" \" + faq_dat.answer)\n",
    "n_faq_ans = len(faq_ans)\n",
    "#ticket question\n",
    "ticket_ques = list(ticket_dat.question)\n",
    "n_ticket_ques = len(ticket_ques)\n",
    "#ticket ans\n",
    "ticket_ans = list(ticket_dat.answer)\n",
    "n_ticket_ans = len(ticket_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model assumption: same embedding for all\n",
    "all_docs = faq_ques + faq_ans + ticket_ques + ticket_ans\n",
    "# Model assumption: two different embeddings\n",
    "all_ans = faq_ans + ticket_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary storing the cut points for the four datasets so we can re-split them after.\n",
    "# use like all_docs[id_dict['faq_ques']] to get all faq questions.\n",
    "id_dict = {\n",
    "    'faq_ques': range(0, n_faq_ques),\n",
    "    'faq_ans': range(n_faq_ques, n_faq_ques + n_faq_ans),\n",
    "    'ticket_ques': range(n_faq_ques + n_faq_ans, n_faq_ques + n_faq_ans + n_ticket_ques),\n",
    "    'ticket_ans': range(n_faq_ques + n_faq_ans + n_ticket_ques, n_faq_ques + n_faq_ans + n_ticket_ques + n_ticket_ans)\n",
    "}\n",
    "all_docs_sep = {\n",
    "    'faq_ques': faq_ques,\n",
    "    'faq_ans': faq_ans,\n",
    "    'ticket_ques': ticket_ques,\n",
    "    'ticket_ans': ticket_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ans_prepro = preprocess_docs_fn(all_ans)\n",
    "ticket_ques_prepro = preprocess_docs_fn(ticket_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2vec model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13641"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading Word2vec model')\n",
    "model_path = '../code/embedding/models/word2vec_ans.model'\n",
    "w2v_ans = Word2Vec.load(model_path)\n",
    "len(w2v_ans.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf on separate datasets, w2v on separate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average 5 most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(all_ans_prepro)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in all_ans_prepro]  # convert corpus to BoW format\n",
    "model_tfidf = TfidfModel(corpus)  # fit model\n",
    "#vector = model_tfidf[corpus[0]]  # apply model to the first corpus document\n",
    "#print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5(dat, corpus, dct, model_w2v, model_tfidf):\n",
    "    if dat == 'faq_ans':\n",
    "        ind = 0\n",
    "        dat = all_ans_prepro[:199]\n",
    "    elif dat == 'ticket_ans':\n",
    "        ind = 199\n",
    "        dat = all_ans_prepro[199:]\n",
    "    else:\n",
    "        ind = 0 \n",
    "        dat = ticket_ques_prepro\n",
    "    mean_ans = np.empty((len(dat), 128), dtype=float)\n",
    "    for i in range(len(dat)):\n",
    "        vector = model_tfidf[corpus[ind]]\n",
    "        vector_s = sorted(vector, key=itemgetter(1), reverse=True)\n",
    "        top5 = vector_s[:5]\n",
    "        top5 = np.asarray(top5, dtype=int)[:,0]\n",
    "        words = np.empty((len(top5), 128), dtype=float)\n",
    "        for j in range(len(top5)):\n",
    "            words[j] = model_w2v[dct[top5[j]]]\n",
    "        mean_ans[i] = np.apply_along_axis(np.mean, 0, words)\n",
    "        ind += 1\n",
    "    return mean_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((199, 128), 4405)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_faq_ans = top5(dat='faq_ans', corpus=corpus, dct=dct, model_w2v=model_w2v, model_tfidf=model_tfidf)\n",
    "mean_faq_ans.shape, len(all_ans_prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted average over 5 most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5_average(dat, corpus, dct, model_w2v, model_tfidf):\n",
    "    if dat == 'faq_ans':\n",
    "        ind = 0\n",
    "        dat = all_ans_prepro[:199]\n",
    "    elif dat == 'ticket_ans':\n",
    "        ind = 199\n",
    "        dat = all_ans_prepro[199:]\n",
    "    else:\n",
    "        ind = 0 \n",
    "        dat = ticket_ques_prepro\n",
    "    mean_ans = np.empty((len(dat), 128), dtype=float)\n",
    "    for i in range(len(dat)):\n",
    "        vector = model_tfidf[corpus[ind]]\n",
    "        vector_s = sorted(vector, key=itemgetter(1), reverse=True)\n",
    "        top5 = vector_s[:5]\n",
    "        top5 = np.asarray(top5, dtype=float)\n",
    "        words = np.empty((len(top5), 128), dtype=float)\n",
    "        for j in range(len(top5)):\n",
    "            words[j] = model_w2v[dct[int(top5[j,0])]]\n",
    "        mean_ans[i] = np.average(words, 0, weights=top5[:,1])\n",
    "        ind += 1\n",
    "    return mean_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((199, 128), 4405)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_faq_ans = top5_average('faq_ans', corpus, dct, model_w2v, model_tfidf)\n",
    "mean_faq_ans.shape, len(all_ans_prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted average over all vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_average(dat, corpus, dct, model_w2v, model_tfidf):\n",
    "    if dat == 'faq_ans':\n",
    "        ind = 0\n",
    "        dat = all_ans_prepro[:199]\n",
    "    elif dat == 'ticket_ans':\n",
    "        ind = 199\n",
    "        dat = all_ans_prepro[199:]\n",
    "    else:\n",
    "        ind = 0 \n",
    "        dat = ticket_ques_prepro\n",
    "    mean_ans = np.empty((len(dat), 128), dtype=float)\n",
    "    for i in range(len(dat)):\n",
    "        vector = np.asarray(model_tfidf[corpus[ind]], dtype=float)\n",
    "        words = np.empty((len(vector), 128), dtype=float)\n",
    "        for j in range(len(vector)):\n",
    "            words[j] = model_w2v[dct[int(vector[j,0])]]\n",
    "        mean_ans[i] = np.average(words, 0, weights=vector[:,1])\n",
    "        ind += 1\n",
    "    return mean_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((199, 128), 4405)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_faq_ans = all_average('faq_ans', corpus, dct, model_w2v, model_tfidf)\n",
    "mean_faq_ans.shape, len(all_ans_prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(mean_ticket_ans, mean_faq_ans):\n",
    "    print('Computing word2vec similarity')\n",
    "\n",
    "    # create matrix with cosine distances from all ticket ans to all faq ans\n",
    "    sim_matrix = cosine_similarity(mean_faq_ans, mean_ticket_ans)\n",
    "\n",
    "    # most similar faq - ticket mapping\n",
    "    FAQ_per_ticket = np.argmax(sim_matrix, axis=0)\n",
    "    strength_FAQ_ticket = np.max(sim_matrix, axis=0)\n",
    "\n",
    "    # small similarities are set to a separate class\n",
    "    thres = 0.2\n",
    "    FAQ_per_ticket[strength_FAQ_ticket < thres] = -1\n",
    "\n",
    "    # some stats\n",
    "    n_unique = len(np.unique(FAQ_per_ticket))\n",
    "    n_nonassigned = np.shape(FAQ_per_ticket[strength_FAQ_ticket < thres])[0]\n",
    "    n_tickets = len(FAQ_per_ticket)\n",
    "    # How many tickets each FAQ is assigned\n",
    "    counts_per_faq = pd.Series(FAQ_per_ticket).value_counts()\n",
    "    #print(counts_per_faq)\n",
    "\n",
    "    output = {\n",
    "        'classes': n_tickets,\n",
    "        'mapping': FAQ_per_ticket\n",
    "    }\n",
    "    print(n_unique, 'classes, with ', round(n_nonassigned / n_tickets, 2), '% non assigned tickets')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE 5 MOST IMPORTANT WORDS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word2vec similarity\n",
      "159 classes, with  0.0 % non assigned tickets\n"
     ]
    }
   ],
   "source": [
    "print('AVERAGE 5 MOST IMPORTANT WORDS')\n",
    "# create doc vector for tickets answers i.e. average over each ticket ans the word2vec vector for each word\n",
    "mean_faq_ans = top5(dat='faq_ans', corpus=corpus, dct=dct, model_w2v=model_w2v, model_tfidf=model_tfidf)\n",
    "# create doc vector for faq ans i.e. average over each faq ans the word2vec vector for each word\n",
    "mean_ticket_ans = top5(dat='ticket_ans', corpus=corpus, dct=dct, model_w2v=model_w2v, model_tfidf=model_tfidf)\n",
    "output = compute_sim(mean_ticket_ans, mean_faq_ans)\n",
    "\n",
    "with open(\"../code/similarity/mappings/map_w2v_tfidf_5a.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(output, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTED AVERAGE OVER 5 MOST IMPORTANT WORDS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word2vec similarity\n",
      "156 classes, with  0.0 % non assigned tickets\n"
     ]
    }
   ],
   "source": [
    "print('WEIGHTED AVERAGE OVER 5 MOST IMPORTANT WORDS')\n",
    "mean_ticket_ans = top5_average(dat='ticket_ans', corpus=corpus, dct=dct, model_w2v=model_w2v, model_tfidf=model_tfidf)\n",
    "mean_faq_ans = top5_average(dat='faq_ans', corpus=corpus, dct=dct, model_w2v=model_w2v, model_tfidf=model_tfidf)\n",
    "\n",
    "output = compute_sim(mean_ticket_ans, mean_faq_ans)\n",
    "\n",
    "with open(\"../code/similarity/mappings/map_w2v_tfidf_5w.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(output, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTED AVERAGE OVER ALL WORDS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word2vec similarity\n",
      "105 classes, with  0.0 % non assigned tickets\n"
     ]
    }
   ],
   "source": [
    "print('WEIGHTED AVERAGE OVER ALL WORDS')\n",
    "mean_ticket_ans = all_average(dat='ticket_ans', corpus=corpus, dct=dct, model_w2v=model_w2v, model_tfidf=model_tfidf)\n",
    "mean_faq_ans = all_average(dat='faq_ans', corpus=corpus, dct=dct, model_w2v=model_w2v, model_tfidf=model_tfidf)\n",
    "\n",
    "output = compute_sim(mean_ticket_ans, mean_faq_ans)\n",
    "\n",
    "with open(\"../code/similarity/mappings/map_w2v_tfidf_all.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(output, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tfidf model\n",
    "model_path = '../code/embedding/models/tfidf_ticket_ques.model'\n",
    "tfidf_ticket = TfidfModel.load(model_path)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model_path = '../code/embedding/models/word2vec_ticket_ques.model'\n",
    "w2v_ticket = Word2Vec.load(model_path)\n",
    "\n",
    "# Recompute dictionary and corpus\n",
    "dct = Dictionary(ticket_ques_prepro)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in ticket_ques_prepro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(mean_ticket_ques, mapping):\n",
    "    \n",
    "    # RANDOM FOREST CLASSIFIER\n",
    "    print('RANDOM FOREST CLASSIFIER')\n",
    "    print('Running CV on Classifier...')\n",
    "    classifier_CV = RandomForestClassifier()\n",
    "    scores = cross_val_score(classifier_CV, mean_ticket_ques, mapping, cv=5)\n",
    "    cv_score = scores.mean()\n",
    "    print('Training Classifier...')\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(X=mean_ticket_ques, y=mapping)\n",
    "    #dump(classifier, 'classifier/models/RF_word2vec.joblib')\n",
    "    train_score = classifier.score(X=mean_ticket_ques, y=mapping)\n",
    "    print('Training Score: {0} \\n Cross Val Score: {1}'.format(train_score, cv_score))\n",
    "    \n",
    "    print('GRADIENT BOOSTING CLASSIEIR')\n",
    "    print('Running CV on Classifier...')\n",
    "    Bclassifier_CV = GradientBoostingClassifier()\n",
    "    scores = cross_val_score(Bclassifier_CV, mean_ticket_ques, mapping, cv=5)\n",
    "    cv_score = scores.mean()\n",
    "    print('Training Classifier...')\n",
    "    Bclassifier = GradientBoostingClassifier()\n",
    "    Bclassifier.fit(X=mean_ticket_ques, y=mapping)\n",
    "    #dump(classifier, 'classifier/models/RF_word2vec.joblib')\n",
    "    train_score = Bclassifier.score(X=mean_ticket_ques, y=mapping)\n",
    "    print('Training Score: {0} \\nCross Val Score: {1}'.format(train_score, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE 5 MOST IMPORTANT WORDS\n",
      "RANDOM FOREST CLASSIFIER\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9854969091773657 \n",
      " Cross Val Score: 0.07964483137596755\n",
      "GRADIENT BOOSTING CLASSIEIR\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9155967665240133 \n",
      "Cross Val Score: 0.07053478484620493\n"
     ]
    }
   ],
   "source": [
    "print('AVERAGE 5 MOST IMPORTANT WORDS')\n",
    "with open('../code/similarity/mappings/map_w2v_tfidf_5a.pkl', 'rb') as fp:\n",
    "    Classes = pickle.load(fp)\n",
    "mapping = Classes['mapping']\n",
    "\n",
    "mean_ticket_ques = top5('ticket_ques', corpus=corpus, dct=dct, model_w2v=w2v_ticket, model_tfidf=tfidf_ticket)\n",
    "\n",
    "classification(mean_ticket_ques, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTED AVERAGE OVER 5 MOST IMPORTANT WORDS\n",
      "RANDOM FOREST CLASSIFIER\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9888254873989539 \n",
      " Cross Val Score: 0.07118744721095863\n",
      "GRADIENT BOOSTING CLASSIEIR\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9210651450309082 \n",
      "Cross Val Score: 0.07391828891184136\n"
     ]
    }
   ],
   "source": [
    "print('WEIGHTED AVERAGE OVER 5 MOST IMPORTANT WORDS')\n",
    "with open('../code/similarity/mappings/map_w2v_tfidf_5w.pkl', 'rb') as fp:\n",
    "    Classes = pickle.load(fp)\n",
    "mapping = Classes['mapping']\n",
    "\n",
    "mean_ticket_ques = top5_average('ticket_ques', corpus=corpus, dct=dct, model_w2v=w2v_ticket, model_tfidf=tfidf_ticket)\n",
    "\n",
    "classification(mean_ticket_ques, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTED AVERAGE OVER ALL WORDS\n",
      "RANDOM FOREST CLASSIFIER\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9835948644793152 \n",
      " Cross Val Score: 0.14091165026354704\n",
      "GRADIENT BOOSTING CLASSIEIR\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.8309557774607703 \n",
      "Cross Val Score: 0.1413967456131142\n"
     ]
    }
   ],
   "source": [
    "print('WEIGHTED AVERAGE OVER ALL WORDS')\n",
    "with open('../code/similarity/mappings/map_w2v_tfidf_all.pkl', 'rb') as fp:\n",
    "    Classes = pickle.load(fp)\n",
    "mapping = Classes['mapping']\n",
    "\n",
    "mean_ticket_ques = top5('ticket_ques', corpus=corpus, dct=dct, model_w2v=w2v_ticket, model_tfidf=tfidf_ticket)\n",
    "\n",
    "classification(mean_ticket_ques, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
