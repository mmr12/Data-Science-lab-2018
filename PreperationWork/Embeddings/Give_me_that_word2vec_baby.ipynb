{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "def remove_ip(s):\n",
    "    # Replace all ip adresses with '<ip>' tag\n",
    "    ip_regexp = r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\"\n",
    "    return re.sub(ip_regexp, '<ip>', s)\n",
    "def remove_email(s):\n",
    "    # Replace all email adresses with '<email>' tag\n",
    "    email_regexp = r\"([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})\"\n",
    "    return re.sub(email_regexp, '<email>', s)\n",
    "def remove_mailto(s):\n",
    "    # Replace all \"<mailto:<email>>\" with <email>. Email adresses should be replaced by remove_email first.\n",
    "    return s.replace(\"<mailto:<email>>\", \"<email>\")\n",
    "def remove_url(s):\n",
    "    # Replace all url's with '<url>' tag\n",
    "    url_regexp = r\"((http|ftp|https):\\/\\/)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "    s = re.sub(url_regexp, '<url>', s)\n",
    "    # Sometimes url's are inside <> so we need to replace <<url>> with <url>\n",
    "    return s.replace(\"<<url>>\", \"<url>\")\n",
    "def remove_punc(s, exceptions):\n",
    "    # Remove all punctuation from string with exceptions in list exceptions\n",
    "    remove = string.punctuation\n",
    "    for exception in exceptions:\n",
    "        remove = remove.replace(exception, \"\")\n",
    "    # Create the pattern\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "\n",
    "    return re.sub(pattern, \"\", s)\n",
    "def remove_custom_stopwords(s, stopwords):\n",
    "    for stopword in stopwords:\n",
    "        s = s.replace(stopword, \"\")\n",
    "    return s\n",
    "def lower_case(s):\n",
    "    return s.lower()\n",
    "def preprocess_sentence_fn(s):\n",
    "    # Preprocess a single sentence to a list of tokens\n",
    "    punc_exceptions = ['<', '>']\n",
    "    custom_stopwords = ['dear', 'sincerely', 'thanks', 'yours', 'regards']\n",
    "    filters = [lower_case,\n",
    "               remove_ip,\n",
    "               remove_email,\n",
    "               remove_mailto,\n",
    "               #remove_url,\n",
    "               lambda x: remove_punc(x, punc_exceptions),\n",
    "               remove_stopwords,\n",
    "               lambda x: remove_custom_stopwords(x, custom_stopwords),\n",
    "               strip_multiple_whitespaces,\n",
    "               stem_text,\n",
    "               strip_numeric]\n",
    "    out = preprocess_string(s, filters=filters)\n",
    "    return out\n",
    "def preprocess_docs_fn(docs):\n",
    "    # Apply preprocess_sentence_fn to a list of sentances (docs) to get a list of lists\n",
    "    return [preprocess_sentence_fn(s) for s in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "ticket_dat = pd.read_csv('../../data/12-04-ticket_dat.csv')\n",
    "faq_dat = pd.read_csv('../../data/12-04-faq_dat.csv')\n",
    "# Replace the NaNs\n",
    "ticket_dat.fillna('', inplace=True)\n",
    "faq_dat.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAQ question\n",
    "faq_ques = list(faq_dat.question)\n",
    "n_faq_ques = len(faq_ques)\n",
    "# FAQ answer\n",
    "faq_ans = list(faq_dat.answer_title + \" \" + faq_dat.answer)\n",
    "n_faq_ans = len(faq_ans)\n",
    "#ticket question\n",
    "ticket_ques = list(ticket_dat.question)\n",
    "n_ticket_ques = len(ticket_ques)\n",
    "#ticket ans\n",
    "ticket_ans = list(ticket_dat.answer)\n",
    "n_ticket_ans = len(ticket_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model assumption: same embedding for all\n",
    "all_docs = faq_ques + faq_ans + ticket_ques + ticket_ans\n",
    "# Model assumption: two different embeddings\n",
    "all_ans = faq_ans + ticket_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary storing the cut points for the four datasets so we can re-split them after.\n",
    "# use like all_docs[id_dict['faq_ques']] to get all faq questions.\n",
    "id_dict = {\n",
    "    'faq_ques': range(0, n_faq_ques),\n",
    "    'faq_ans': range(n_faq_ques, n_faq_ques + n_faq_ans),\n",
    "    'ticket_ques': range(n_faq_ques + n_faq_ans, n_faq_ques + n_faq_ans + n_ticket_ques),\n",
    "    'ticket_ans': range(n_faq_ques + n_faq_ans + n_ticket_ques, n_faq_ques + n_faq_ans + n_ticket_ques + n_ticket_ans)\n",
    "}\n",
    "all_docs_sep = {\n",
    "    'faq_ques': faq_ques,\n",
    "    'faq_ans': faq_ans,\n",
    "    'ticket_ques': ticket_ques,\n",
    "    'ticket_ans': ticket_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_prepro = preprocess_docs_fn(all_docs)\n",
    "all_ans_prepro = preprocess_docs_fn(all_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#FOR DEBUGGING PURPOSE\n",
    "\n",
    "#check if datasets contain empty strings\n",
    "faq_ques_prepro = preprocess_docs_fn(faq_ques)\n",
    "for i in range(len(faq_ques_prepro)):\n",
    "    if not faq_ques_prepro[i]:\n",
    "        print('faq question {}'.format(i))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec embedding model already existing\n"
     ]
    }
   ],
   "source": [
    "#ALL ANSWERs\n",
    "# checking if embedding model already exists\n",
    "exists = os.path.isfile('../../code/embedding/models/word2vec_ans.model')\n",
    "if exists:\n",
    "    print('Word2vec embedding model already existing')\n",
    "# Create word embedding model\n",
    "else:\n",
    "    print('Training word2vec on all answers')\n",
    "    word_path = \"../../code/embedding/models/word2vec_ans.model\"\n",
    "    word_tempfile = get_tmpfile(word_path)\n",
    "    word_model = Word2Vec(all_ans_prepro, size=128, window=5, min_count=1, workers=4)\n",
    "    word_model.save(word_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_ques_prepro = preprocess_docs_fn(ticket_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec embedding model already existing\n"
     ]
    }
   ],
   "source": [
    "#TICKET QUESTIONS\n",
    "exists = os.path.isfile('../../code/embedding/models/word2vec_ticket_ques.model')\n",
    "if exists:\n",
    "    print('Word2vec embedding model already existing')\n",
    "else:\n",
    "    #not checking if already exists because if the first doesn't this won't either\n",
    "    print('Training word2vec on ticket questions')\n",
    "    word_path = \"../../code/embedding/models/word2vec_ticket_ques.model\"\n",
    "    word_tempfile = get_tmpfile(word_path)\n",
    "    word_model = Word2Vec(ticket_ques_prepro, size=128, window=5, min_count=1, workers=4)\n",
    "    word_model.save(word_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2vec model\n"
     ]
    }
   ],
   "source": [
    "print('Loading Word2vec model')\n",
    "model_path = '../../code/embedding/models/word2vec_ans.model'\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_emb(dat):\n",
    "    mean_ans = np.empty((len(dat), 128), dtype=float)\n",
    "    for j in range(len(dat)):\n",
    "        sentence = dat[j]\n",
    "        words = np.empty((len(sentence), 128), dtype=float)\n",
    "        for i in range(len(sentence)):\n",
    "            words[i] = model[sentence[i]]\n",
    "        mean_ans[j] = np.apply_along_axis(np.mean, 0, words)\n",
    "    return mean_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(mean_ticket_ans, mean_faq_ans):\n",
    "    print('Computing word2vec similarity')\n",
    "\n",
    "    # create matrix with cosine distances from all ticket ans to all faq ans\n",
    "    sim_matrix = cosine_similarity(mean_faq_ans, mean_ticket_ans)\n",
    "\n",
    "    # most similar faq - ticket mapping\n",
    "    FAQ_per_ticket = np.argmax(sim_matrix, axis=0)\n",
    "    strength_FAQ_ticket = np.max(sim_matrix, axis=0)\n",
    "\n",
    "    # small similarities are set to a separate class\n",
    "    thres = 0.2\n",
    "    FAQ_per_ticket[strength_FAQ_ticket < thres] = -1\n",
    "\n",
    "    # some stats\n",
    "    n_unique = len(np.unique(FAQ_per_ticket))\n",
    "    n_nonassigned = np.shape(FAQ_per_ticket[strength_FAQ_ticket < thres])[0]\n",
    "    n_tickets = len(FAQ_per_ticket)\n",
    "    # How many tickets each FAQ is assigned\n",
    "    counts_per_faq = pd.Series(FAQ_per_ticket).value_counts()\n",
    "    #print(counts_per_faq)\n",
    "\n",
    "    output = {\n",
    "        'classes': n_tickets,\n",
    "        'mapping': FAQ_per_ticket\n",
    "    }\n",
    "    print(n_unique, 'classes, with ', round(n_nonassigned / n_tickets, 2), '% non assigned tickets')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word2vec similarity\n",
      "97 classes, with  0.0 % non assigned tickets\n"
     ]
    }
   ],
   "source": [
    "mean_ticket_ans = doc_emb(all_ans_prepro[len(faq_ans):len(all_ans)])\n",
    "mean_faq_ans = doc_emb(all_ans_prepro[0:len(faq_ans)])\n",
    "\n",
    "output = compute_sim(mean_ticket_ans=mean_ticket_ans, mean_faq_ans=mean_faq_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug: print the vectors with 0 \n",
    "'''\n",
    "for i in range(len(mean_ticket_ans)):\n",
    "    zero = np.count_nonzero(mean_ticket_ans[i])\n",
    "    if zero != 128: \n",
    "        print(i)\n",
    "        \n",
    "for i in range(len(mean_faq_ans)):\n",
    "    zero = np.count_nonzero(mean_ticket_ans[i])\n",
    "    if zero != 128: \n",
    "        print(i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(mean_ticket_ques, mapping):\n",
    "    \n",
    "    # RANDOM FOREST CLASSIFIER\n",
    "    print('RANDOM FOREST CLASSIFIER')\n",
    "    print('Running CV on Classifier...')\n",
    "    classifier_CV = RandomForestClassifier()\n",
    "    scores = cross_val_score(classifier_CV, mean_ticket_ques, mapping, cv=5)\n",
    "    cv_score = scores.mean()\n",
    "    print('Training Classifier...')\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(X=mean_ticket_ques, y=mapping)\n",
    "    #dump(classifier, 'classifier/models/RF_word2vec.joblib')\n",
    "    train_score = classifier.score(X=mean_ticket_ques, y=mapping)\n",
    "    print('Training Score: {0} \\n Cross Val Score: {1}'.format(train_score, cv_score))\n",
    "    \n",
    "    print('GRADIENT BOOSTING CLASSIEIR')\n",
    "    print('Running CV on Classifier...')\n",
    "    Bclassifier_CV = GradientBoostingClassifier()\n",
    "    scores = cross_val_score(Bclassifier_CV, mean_ticket_ques, mapping, cv=5)\n",
    "    cv_score = scores.mean()\n",
    "    print('Training Classifier...')\n",
    "    Bclassifier = GradientBoostingClassifier()\n",
    "    Bclassifier.fit(X=mean_ticket_ques, y=mapping)\n",
    "    #dump(classifier, 'classifier/models/RF_word2vec.joblib')\n",
    "    train_score = Bclassifier.score(X=mean_ticket_ques, y=mapping)\n",
    "    print('Training Score: {0} \\nCross Val Score: {1}'.format(train_score, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Word2Vec model\n",
    "model_path = '../../code/embedding/models/word2vec_ticket_ques.model'\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticket_question_embeddings = doc_emb(ticket_ques_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST CLASSIFIER\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9888254873989539 \n",
      " Cross Val Score: 0.15699187718282617\n",
      "GRADIENT BOOSTING CLASSIEIR\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9365192582025678 \n",
      "Cross Val Score: 0.1620538609730758\n"
     ]
    }
   ],
   "source": [
    "with open('../../code/similarity/mappings/ticket_faq_map_word2vec.pkl', 'rb') as fp:\n",
    "    Classes = pickle.load(fp)\n",
    "mapping = Classes['mapping']\n",
    "\n",
    "ticket_question_embeddings = doc_emb(ticket_ques_prepro)\n",
    "\n",
    "classification(ticket_question_embeddings, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2vec model\n"
     ]
    }
   ],
   "source": [
    "print('Loading Word2vec model')\n",
    "model_path = '../../code/embedding/models/word2vec_all.model'\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_emb_one(name):\n",
    "    mean_ans = np.empty((len(id_dict[name]), 128), dtype=float)\n",
    "    for j in id_dict[name]:\n",
    "        sentence = all_docs_prepro[j]\n",
    "        words = np.empty((len(sentence), 128), dtype=float)\n",
    "        for i in range(len(sentence)):\n",
    "            words[i] = model[sentence[i]]\n",
    "        mean_ans[j - id_dict[name][0]] = np.apply_along_axis(np.mean, 0, words)\n",
    "    return mean_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word2vec similarity\n",
      "Computing word2vec similarity\n",
      "109 classes, with  0.0 % non assigned tickets\n"
     ]
    }
   ],
   "source": [
    "print('Computing word2vec similarity')\n",
    "#create doc vector for tickets answers i.e. average over each ticket ans the word2vec vector for each word\n",
    "mean_ticket_ans = doc_emb_one('ticket_ans')\n",
    "#create doc vector for faq ans i.e. average over each faq ans the word2vec vector for each word\n",
    "mean_faq_ans = doc_emb_one('faq_ans')\n",
    "\n",
    "output = compute_sim(mean_ticket_ans=mean_ticket_ans, mean_faq_ans=mean_faq_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST CLASSIFIER\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9866856871136471 \n",
      " Cross Val Score: 0.3335240993573032\n",
      "GRADIENT BOOSTING CLASSIEIR\n",
      "Running CV on Classifier...\n",
      "Training Classifier...\n",
      "Training Score: 0.9317641464574418 \n",
      "Cross Val Score: 0.31579189729889257\n"
     ]
    }
   ],
   "source": [
    "mapping = output['mapping']\n",
    "\n",
    "ticket_question_embeddings = doc_emb_one('ticket_ques')\n",
    "\n",
    "classification(ticket_question_embeddings, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
