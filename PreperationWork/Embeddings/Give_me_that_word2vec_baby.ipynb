{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "def remove_ip(s):\n",
    "    # Replace all ip adresses with '<ip>' tag\n",
    "    ip_regexp = r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\"\n",
    "    return re.sub(ip_regexp, '<ip>', s)\n",
    "def remove_email(s):\n",
    "    # Replace all email adresses with '<email>' tag\n",
    "    email_regexp = r\"([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})\"\n",
    "    return re.sub(email_regexp, '<email>', s)\n",
    "def remove_mailto(s):\n",
    "    # Replace all \"<mailto:<email>>\" with <email>. Email adresses should be replaced by remove_email first.\n",
    "    return s.replace(\"<mailto:<email>>\", \"<email>\")\n",
    "def remove_url(s):\n",
    "    # Replace all url's with '<url>' tag\n",
    "    url_regexp = r\"((http|ftp|https):\\/\\/)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "    s = re.sub(url_regexp, '<url>', s)\n",
    "    # Sometimes url's are inside <> so we need to replace <<url>> with <url>\n",
    "    return s.replace(\"<<url>>\", \"<url>\")\n",
    "def remove_punc(s, exceptions):\n",
    "    # Remove all punctuation from string with exceptions in list exceptions\n",
    "    remove = string.punctuation\n",
    "    for exception in exceptions:\n",
    "        remove = remove.replace(exception, \"\")\n",
    "    # Create the pattern\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "\n",
    "    return re.sub(pattern, \"\", s)\n",
    "def remove_custom_stopwords(s, stopwords):\n",
    "    for stopword in stopwords:\n",
    "        s = s.replace(stopword, \"\")\n",
    "    return s\n",
    "def lower_case(s):\n",
    "    return s.lower()\n",
    "def preprocess_sentence_fn(s):\n",
    "    # Preprocess a single sentence to a list of tokens\n",
    "    punc_exceptions = ['<', '>']\n",
    "    custom_stopwords = ['dear', 'sincerely', 'thanks', 'yours', 'regards']\n",
    "    filters = [lower_case,\n",
    "               remove_ip,\n",
    "               remove_email,\n",
    "               remove_mailto,\n",
    "               #remove_url,\n",
    "               lambda x: remove_punc(x, punc_exceptions),\n",
    "               remove_stopwords,\n",
    "               lambda x: remove_custom_stopwords(x, custom_stopwords),\n",
    "               strip_multiple_whitespaces,\n",
    "               stem_text,\n",
    "               strip_numeric]\n",
    "    out = preprocess_string(s, filters=filters)\n",
    "    return out\n",
    "def preprocess_docs_fn(docs):\n",
    "    # Apply preprocess_sentence_fn to a list of sentances (docs) to get a list of lists\n",
    "    return [preprocess_sentence_fn(s) for s in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_ticket_ans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-a283f8c469ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#debug: print the vectors with 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_ticket_ans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mzero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_ticket_ans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mzero\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_ticket_ans' is not defined"
     ]
    }
   ],
   "source": [
    "#debug: print the vectors with 0 \n",
    "\n",
    "for i in range(len(mean_ticket_ans)):\n",
    "    zero = np.count_nonzero(mean_ticket_ans[i])\n",
    "    if zero != 128: \n",
    "        print(i)\n",
    "        \n",
    "for i in range(len(mean_faq_ans)):\n",
    "    zero = np.count_nonzero(mean_ticket_ans[i])\n",
    "    if zero != 128: \n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove empty strings due to preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "ticket_dat = pd.read_csv('../../data/12-04-ticket_dat.csv')\n",
    "faq_dat = pd.read_csv('../../data/12-04-faq_dat.csv')\n",
    "# Replace the NaNs\n",
    "ticket_dat.fillna('', inplace=True)\n",
    "faq_dat.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAQ question\n",
    "faq_ques = list(faq_dat.question)\n",
    "n_faq_ques = len(faq_ques)\n",
    "# FAQ answer\n",
    "faq_ans = list(faq_dat.answer_title + \" \" + faq_dat.answer)\n",
    "n_faq_ans = len(faq_ans)\n",
    "#ticket question\n",
    "ticket_ques = list(ticket_dat.question)\n",
    "n_ticket_ques = len(ticket_ques)\n",
    "#ticket ans\n",
    "ticket_ans = list(ticket_dat.answer)\n",
    "n_ticket_ans = len(ticket_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model assumption: same embedding for all\n",
    "all_docs = faq_ques + faq_ans + ticket_ques + ticket_ans\n",
    "# Model assumption: two different embeddings\n",
    "all_ans = faq_ans + ticket_ans\n",
    "\n",
    "# create a dictionary storing the cut points for the four datasets so we can re-split them after.\n",
    "# use like all_docs[id_dict['faq_ques']] to get all faq questions.\n",
    "id_dict = {\n",
    "    'faq_ques': range(0, n_faq_ques),\n",
    "    'faq_ans': range(n_faq_ques, n_faq_ques + n_faq_ans),\n",
    "    'ticket_ques': range(n_faq_ques + n_faq_ans, n_faq_ques + n_faq_ans + n_ticket_ques),\n",
    "    'ticket_ans': range(n_faq_ques + n_faq_ans + n_ticket_ques, n_faq_ques + n_faq_ans + n_ticket_ques + n_ticket_ans)\n",
    "}\n",
    "all_docs_sep = {\n",
    "    'faq_ques': faq_ques,\n",
    "    'faq_ans': faq_ans,\n",
    "    'ticket_ques': ticket_ques,\n",
    "    'ticket_ans': ticket_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_prepro = preprocess_docs_fn(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty docs 14\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_docs_prepro)):\n",
    "    if not all_docs_prepro[i]:\n",
    "        print('empty docs {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faq question 14\n"
     ]
    }
   ],
   "source": [
    "#check if datasets contain empty strings\n",
    "faq_ques_prepro = preprocess_docs_fn(faq_ques)\n",
    "for i in range(len(faq_ques_prepro)):\n",
    "    if not faq_ques_prepro[i]:\n",
    "        print('faq question {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_ques[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_ans_prepro = preprocess_docs_fn(faq_ans)\n",
    "for i in range(len(faq_ans_prepro)):\n",
    "    if not faq_ans_prepro[i]:\n",
    "        print('faq answer {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_ques_prepro = preprocess_docs_fn(ticket_ques)\n",
    "for i in range(len(ticket_ques_prepro)):\n",
    "    if not ticket_ques_prepro[i]:\n",
    "        print('ticket question {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ticket_ans_prepro = preprocess_docs_fn(ticket_ans)\n",
    "for i in range(len(ticket_ans_prepro)):\n",
    "    if not ticket_ans_prepro[i]:\n",
    "        print('ticket answer {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
