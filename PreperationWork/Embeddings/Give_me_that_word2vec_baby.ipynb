{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "def remove_ip(s):\n",
    "    # Replace all ip adresses with '<ip>' tag\n",
    "    ip_regexp = r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\"\n",
    "    return re.sub(ip_regexp, '<ip>', s)\n",
    "def remove_email(s):\n",
    "    # Replace all email adresses with '<email>' tag\n",
    "    email_regexp = r\"([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})\"\n",
    "    return re.sub(email_regexp, '<email>', s)\n",
    "def remove_mailto(s):\n",
    "    # Replace all \"<mailto:<email>>\" with <email>. Email adresses should be replaced by remove_email first.\n",
    "    return s.replace(\"<mailto:<email>>\", \"<email>\")\n",
    "def remove_url(s):\n",
    "    # Replace all url's with '<url>' tag\n",
    "    url_regexp = r\"((http|ftp|https):\\/\\/)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "    s = re.sub(url_regexp, '<url>', s)\n",
    "    # Sometimes url's are inside <> so we need to replace <<url>> with <url>\n",
    "    return s.replace(\"<<url>>\", \"<url>\")\n",
    "def remove_punc(s, exceptions):\n",
    "    # Remove all punctuation from string with exceptions in list exceptions\n",
    "    remove = string.punctuation\n",
    "    for exception in exceptions:\n",
    "        remove = remove.replace(exception, \"\")\n",
    "    # Create the pattern\n",
    "    pattern = r\"[{}]\".format(remove)\n",
    "\n",
    "    return re.sub(pattern, \"\", s)\n",
    "def remove_custom_stopwords(s, stopwords):\n",
    "    for stopword in stopwords:\n",
    "        s = s.replace(stopword, \"\")\n",
    "    return s\n",
    "def lower_case(s):\n",
    "    return s.lower()\n",
    "def preprocess_sentence_fn(s):\n",
    "    # Preprocess a single sentence to a list of tokens\n",
    "    punc_exceptions = ['<', '>']\n",
    "    custom_stopwords = ['dear', 'sincerely', 'thanks', 'yours', 'regards']\n",
    "    filters = [lower_case,\n",
    "               remove_ip,\n",
    "               remove_email,\n",
    "               remove_mailto,\n",
    "               #remove_url,\n",
    "               lambda x: remove_punc(x, punc_exceptions),\n",
    "               remove_stopwords,\n",
    "               lambda x: remove_custom_stopwords(x, custom_stopwords),\n",
    "               strip_multiple_whitespaces,\n",
    "               stem_text,\n",
    "               strip_numeric]\n",
    "    out = preprocess_string(s, filters=filters)\n",
    "    return out\n",
    "def preprocess_docs_fn(docs):\n",
    "    # Apply preprocess_sentence_fn to a list of sentances (docs) to get a list of lists\n",
    "    return [preprocess_sentence_fn(s) for s in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "ticket_dat = pd.read_csv('../../data/12-04-ticket_dat.csv')\n",
    "faq_dat = pd.read_csv('../../data/12-04-faq_dat.csv')\n",
    "# Replace the NaNs\n",
    "ticket_dat.fillna('', inplace=True)\n",
    "faq_dat.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAQ question\n",
    "faq_ques = list(faq_dat.question)\n",
    "n_faq_ques = len(faq_ques)\n",
    "# FAQ answer\n",
    "faq_ans = list(faq_dat.answer_title + \" \" + faq_dat.answer)\n",
    "n_faq_ans = len(faq_ans)\n",
    "#ticket question\n",
    "ticket_ques = list(ticket_dat.question)\n",
    "n_ticket_ques = len(ticket_ques)\n",
    "#ticket ans\n",
    "ticket_ans = list(ticket_dat.answer)\n",
    "n_ticket_ans = len(ticket_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model assumption: same embedding for all\n",
    "all_docs = faq_ques + faq_ans + ticket_ques + ticket_ans\n",
    "# Model assumption: two different embeddings\n",
    "all_ans = faq_ans + ticket_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary storing the cut points for the four datasets so we can re-split them after.\n",
    "# use like all_docs[id_dict['faq_ques']] to get all faq questions.\n",
    "id_dict = {\n",
    "    'faq_ques': range(0, n_faq_ques),\n",
    "    'faq_ans': range(n_faq_ques, n_faq_ques + n_faq_ans),\n",
    "    'ticket_ques': range(n_faq_ques + n_faq_ans, n_faq_ques + n_faq_ans + n_ticket_ques),\n",
    "    'ticket_ans': range(n_faq_ques + n_faq_ans + n_ticket_ques, n_faq_ques + n_faq_ans + n_ticket_ques + n_ticket_ans)\n",
    "}\n",
    "all_docs_sep = {\n",
    "    'faq_ques': faq_ques,\n",
    "    'faq_ans': faq_ans,\n",
    "    'ticket_ques': ticket_ques,\n",
    "    'ticket_ans': ticket_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_prepro = preprocess_docs_fn(all_docs)\n",
    "all_ans_prepro = preprocess_docs_fn(all_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR DEBUGGING PURPOSE\n",
    "\n",
    "#check if datasets contain empty strings\n",
    "faq_ques_prepro = preprocess_docs_fn(faq_ques)\n",
    "for i in range(len(faq_ques_prepro)):\n",
    "    if not faq_ques_prepro[i]:\n",
    "        print('faq question {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec embedding model already existing\n"
     ]
    }
   ],
   "source": [
    "#ALL ANSWERs\n",
    "# checking if embedding model already exists\n",
    "exists = os.path.isfile('../../code/embedding/models/word2vec_ans.model')\n",
    "if exists:\n",
    "    print('Word2vec embedding model already existing')\n",
    "# Create word embedding model\n",
    "else:\n",
    "    print('Training word2vec on all answers')\n",
    "    word_path = \"../../code/embedding/models/word2vec_ans.model\"\n",
    "    word_tempfile = get_tmpfile(word_path)\n",
    "    word_model = Word2Vec(all_ans_prepro, size=128, window=5, min_count=1, workers=4)\n",
    "    word_model.save(word_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_ques_prepro = preprocess_docs_fn(ticket_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec on ticket questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "#TICKET QUESTIONS\n",
    "exists = os.path.isfile('../../code/embedding/models/word2vec_ticket_ques.model')\n",
    "if exists:\n",
    "    print('Word2vec embedding model already existing')\n",
    "else:\n",
    "    #not checking if already exists because if the first doesn't this won't either\n",
    "    print('Training word2vec on ticket questions')\n",
    "    word_path = \"../../code/embedding/models/word2vec_ticket_ques.model\"\n",
    "    word_tempfile = get_tmpfile(word_path)\n",
    "    word_model = Word2Vec(ticket_ques_prepro, size=128, window=5, min_count=1, workers=4)\n",
    "    word_model.save(word_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2vec model\n"
     ]
    }
   ],
   "source": [
    "print('Loading Word2vec model')\n",
    "model_path = '../../code/embedding/models/word2vec_ans.model'\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_emb(dat):\n",
    "    mean_ans = np.empty((len(dat), 128), dtype=float)\n",
    "    for j in range(len(dat)):\n",
    "        sentence = dat[j]\n",
    "        words = np.empty((len(sentence), 128), dtype=float)\n",
    "        for i in range(len(sentence)):\n",
    "            words[i] = model[sentence[i]]\n",
    "        mean_ans[j] = np.apply_along_axis(np.mean, 0, words)\n",
    "    return mean_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mean_ticket_ans = doc_emb(all_ans_prepro[len(faq_ans):len(all_ans)])\n",
    "#create doc vector for faq ans i.e. average over each faq ans the word2vec vector for each word\n",
    "mean_faq_ans = doc_emb(all_ans_prepro[0:len(faq_ans)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_matrix = cosine_similarity(mean_faq_ans, mean_ticket_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most similar faq - ticket mapping\n",
    "FAQ_per_ticket = np.argmax(sim_matrix, axis=0)\n",
    "strength_FAQ_ticket = np.max(sim_matrix, axis=0)\n",
    "\n",
    "#small similarities are set to a separate class\n",
    "thres = 0.2\n",
    "FAQ_per_ticket[strength_FAQ_ticket < thres] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug: print the vectors with 0 \n",
    "'''\n",
    "for i in range(len(mean_ticket_ans)):\n",
    "    zero = np.count_nonzero(mean_ticket_ans[i])\n",
    "    if zero != 128: \n",
    "        print(i)\n",
    "        \n",
    "for i in range(len(mean_faq_ans)):\n",
    "    zero = np.count_nonzero(mean_ticket_ans[i])\n",
    "    if zero != 128: \n",
    "        print(i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Word2Vec model\n",
    "model_path = '../../code/embedding/models/word2vec_ticket_ques.model'\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_emb(dat):\n",
    "    mean_ans = np.empty((len(dat), 128), dtype=float)\n",
    "    for j in range(len(dat)):\n",
    "        sentence = dat[j]\n",
    "        words = np.empty((len(sentence), 128), dtype=float)\n",
    "        for i in range(len(sentence)):\n",
    "            words[i] = model[sentence[i]]\n",
    "        mean_ans[j] = np.apply_along_axis(np.mean, 0, words)\n",
    "    return mean_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "ticket_question_embeddings = doc_emb(ticket_ques_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CV on Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Running CV on Classifier...')\n",
    "classifier_CV = RandomForestClassifier()\n",
    "scores = cross_val_score(classifier_CV, ticket_question_embeddings, FAQ_per_ticket, cv=5)\n",
    "cv_score = scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\costanza\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9871611982881597 \n",
      " Cross Val Score: 0.21132894467194116\n",
      "###############\n"
     ]
    }
   ],
   "source": [
    "print('Training Classifier...')\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X=ticket_question_embeddings, y=FAQ_per_ticket)\n",
    "#dump(classifier, 'classifier/models/RF_word2vec.joblib')\n",
    "train_score = classifier.score(X=ticket_question_embeddings, y=FAQ_per_ticket)\n",
    "\n",
    "print('Training Score: {0} \\n Cross Val Score: {1}'.format(train_score, cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
